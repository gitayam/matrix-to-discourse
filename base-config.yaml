#base-config.yaml is the main configuration file for the bot. It contains the settings for the bot, including the bot's username, password, and the server to connect to. It also contains the settings for the Discourse integration, the AI model, and the triggers for the bot's commands.
###### BOT Configuration ######
# The bot will only respond to messages that start with this prefix along with "!" such as !command
search_trigger="fsearch"
post_trigger="fpost"
help_trigger="fhelp"
url_post_trigger="furl"
###### Discourse Configuration ######
# To get your API key, go to https://discourse.example.com/admin/api/keys
# replace "discourse.example.com" with your Discourse URL
discourse_api_key: "discourse_api_key"
discourse_api_username: "discourse_api_username"
discourse_base_url: "https://discourse.example.com"
# To get your category ID, go to https://discourse.example.com/c/your-category-name.json
matrix_to_discourse_topic:
  "!roomID1:server": "topic_id_1"
  "!roomID2:server": "topic_id_2"
unsorted_category_id: topic_id_9 # The category ID for the unsorted category
######### AI Model Configuration #########
## GPT is used for creating titles and descriptions for new topics based on a post.


###### AI Model Configuration ######
# Choose the AI model type: "openai", "google", "local", or "none"
# If ai_model_type is set to "none", no AI model will be used
ai_model_type: "none"
# default to none to give user a chance to set it to openai, google, or local in the config file.
###### OpenAI API Configuration ######
# Use these settings if ai_model_type is set to "openai"
# To get your API key, sign up at https://platform.openai.com/signup or log in at https://platform.openai.com/login
openai:
  api_key: "gpt_api_key"
  api_endpoint: "https://api.openai.com/v1/chat/completions"
  model: "gpt-4o-mini"
  max_tokens: 3000
  temperature: 1

  ###### Local LLM Configuration ######
# Use these settings if ai_model_type is set to "local"
# Note: This could be a locally running model or a model accessed via a local server like Ollama
local_llm:
  # For a local model file:
  # model_path is typically a file with a .gguf extension
  model_path: "/path/to/your/local/model"
  # For a local server (e.g., Ollama):
  # api_endpoint: "http://localhost:11434/api/chat"
  # model: "llama2"
  # Add any other necessary configuration for your local LLM
  # For example:
  # max_tokens: 3000
  # temperature: 1

###### Google Gemini Configuration ######
# Use these settings if ai_model_type is set to "google"
google:
  api_key: "google_api_key"
  api_endpoint: "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent"
  model: "gemini-1.5-pro"
  max_tokens: 3000
  temperature: 1

###### URL Pattern Configuration ######
# The bot will only respond to URLs that match these patterns
url_patterns:
  - 'https?://example\.com/.*'
  - 'https?://anotherdomain\.org/articles/.*'

